{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd01f9a8",
   "metadata": {},
   "source": [
    "# Logistic Regression Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221bbfcc",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "**What is Logistic Regression, and how does it differ from Linear Regression?**\n",
    "\n",
    "**Answer (concise):**\n",
    "Logistic Regression is a classification algorithm used to predict categorical outcomes (most commonly binary).  \n",
    "- It models the probability that an input belongs to a particular class using the logistic (sigmoid) function, producing outputs in [0,1].  \n",
    "- Decision rule: predict class 1 if probability ≥ 0.5 (or another chosen threshold), else class 0.\n",
    "\n",
    "**Difference from Linear Regression:**\n",
    "- Linear Regression predicts continuous values (real numbers); Logistic Regression predicts class probabilities.\n",
    "- Linear output is unbounded; logistic output is bounded between 0 and 1 via the sigmoid.\n",
    "- Loss: Linear Regression typically uses MSE; Logistic Regression uses log-loss / cross-entropy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99c5820",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "**Explain the role of the Sigmoid function in Logistic Regression.**\n",
    "\n",
    "**Answer (concise):**\n",
    "The sigmoid (logistic) function maps real-valued model scores (z = w·x + b) to the range (0,1) as:\n",
    "\\[\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "\\]\n",
    "This value is interpreted as the probability of the positive class. The sigmoid ensures outputs are probabilities and enables the use of cross-entropy loss for training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a0f907",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "**What is Regularization in Logistic Regression and why is it needed?**\n",
    "\n",
    "**Answer (concise):**\n",
    "Regularization penalizes large weights to reduce overfitting and improve generalization.\n",
    "- **L2 (Ridge)**: adds \\(\\lambda ||w||_2^2\\) penalty; shrinks weights continuously.\n",
    "- **L1 (Lasso)**: adds \\(\\lambda ||w||_1\\) penalty; can set some weights exactly to zero (feature selection).\n",
    "Regularization helps when features are many, collinear, or when model overfits training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f33076",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "**What are some common evaluation metrics for classification models, and why are they important?**\n",
    "\n",
    "**Answer (concise):**\n",
    "- **Accuracy**: fraction of correct predictions. Good when classes balanced.\n",
    "- **Precision**: TP / (TP + FP). Useful when false positives are costly.\n",
    "- **Recall (Sensitivity)**: TP / (TP + FN). Useful when false negatives are costly.\n",
    "- **F1-score**: harmonic mean of precision & recall; good single-number metric for imbalance.\n",
    "- **ROC-AUC**: area under ROC curve (probability that a random positive ranks above a random negative).\n",
    "- **Confusion Matrix**: counts TP, TN, FP, FN — helps analyze types of errors.\n",
    "Choosing metrics matters depending on class balance and business cost of error types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce1d625e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9649122807017544\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.93      0.95        42\n",
      "           1       0.96      0.99      0.97        72\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.97      0.96      0.96       114\n",
      "weighted avg       0.97      0.96      0.96       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Question 5\n",
    "# Load a dataset, split, train LogisticRegression and print accuracy.\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "model = LogisticRegression(max_iter=10000, solver='lbfgs')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b7a24b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (L2): 0.9649122807017544\n",
      "\n",
      "Top 10 features by absolute coefficient:\n",
      "Feature 26: coef = -1.3212\n",
      "Feature 11: coef = 1.0810\n",
      "Feature 0: coef = 0.8096\n",
      "Feature 28: coef = -0.7855\n",
      "Feature 25: coef = -0.7577\n",
      "Feature 27: coef = -0.5526\n",
      "Feature 6: coef = -0.4553\n",
      "Feature 21: coef = -0.3742\n",
      "Feature 24: coef = -0.3188\n",
      "Feature 8: coef = -0.3054\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Question 6\n",
    "# Train Logistic Regression with L2 regularization (default) and print coefficients and accuracy.\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# L2 regularization is the default (penalty='l2'). C controls inverse strength.\n",
    "model_l2 = LogisticRegression(penalty='l2', C=1.0, max_iter=10000, solver='lbfgs')\n",
    "model_l2.fit(X_train, y_train)\n",
    "y_pred = model_l2.predict(X_test)\n",
    "\n",
    "print(\"Accuracy (L2):\", accuracy_score(y_test, y_pred))\n",
    "# Coefficients\n",
    "coef = model_l2.coef_.ravel()\n",
    "# Print top 10 absolute coefficient ranks\n",
    "inds = np.argsort(np.abs(coef))[::-1][:10]\n",
    "print(\"\\nTop 10 features by absolute coefficient:\")\n",
    "for i in inds:\n",
    "    print(f\"Feature {i}: coef = {coef[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec27cb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report (ovr):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      0.80      0.89        10\n",
      "           2       0.83      1.00      0.91        10\n",
      "\n",
      "    accuracy                           0.93        30\n",
      "   macro avg       0.94      0.93      0.93        30\n",
      "weighted avg       0.94      0.93      0.93        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Question 7\n",
    "# Multiclass logistic regression using multi_class='ovr' and print classification report.\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "model_ovr = LogisticRegression(multi_class='ovr', max_iter=10000, solver='lbfgs')\n",
    "model_ovr.fit(X_train, y_train)\n",
    "y_pred = model_ovr.predict(X_test)\n",
    "\n",
    "print(\"Classification report (ovr):\\n\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b81e032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'C': 10, 'penalty': 'l2'}\n",
      "Validation accuracy with best params: 0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Question 8\n",
    "# GridSearchCV to tune C and penalty for Logistic Regression\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l2'],  # using l2 with lbfgs; l1 requires solver='saga' or 'liblinear'\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(LogisticRegression(max_iter=10000, solver='lbfgs'), param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", clf.best_params_)\n",
    "best_model = clf.best_estimator_\n",
    "y_pred = best_model.predict(X_val)\n",
    "print(\"Validation accuracy with best params:\", accuracy_score(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d709946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy without scaling: 0.9649122807017544\n",
      "Accuracy with scaling: 0.9824561403508771\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Question 9\n",
    "# Compare accuracy with and without StandardScaler\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Without scaling\n",
    "model_raw = LogisticRegression(max_iter=10000, solver='lbfgs')\n",
    "model_raw.fit(X_train, y_train)\n",
    "acc_raw = accuracy_score(y_test, model_raw.predict(X_test))\n",
    "\n",
    "# With scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "model_scaled = LogisticRegression(max_iter=10000, solver='lbfgs')\n",
    "model_scaled.fit(X_train_s, y_train)\n",
    "acc_scaled = accuracy_score(y_test, model_scaled.predict(X_test_s))\n",
    "\n",
    "print(\"Accuracy without scaling:\", acc_raw)\n",
    "print(\"Accuracy with scaling:\", acc_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba97d2a9",
   "metadata": {},
   "source": [
    "# Question 10\n",
    "**Real-world approach for an imbalanced e-commerce dataset (5% responders)**\n",
    "\n",
    "**Answer (detailed steps):**\n",
    "1. **Data understanding & cleaning**: validate labels, handle missing values, convert categorical features (one-hot / target encoding).\n",
    "2. **Feature engineering**: create interaction features, customer recency/frequency/monetary (RFM), embedding categorical high-cardinality features if needed.\n",
    "3. **Train-test split**: keep a stratified split so minority class proportion preserved (e.g., `train_test_split(..., stratify=y)`).\n",
    "4. **Scaling**: scale numerical features (StandardScaler) since logistic regression is sensitive to feature scales.\n",
    "5. **Handling imbalance**:\n",
    "   - Use class weights in LogisticRegression (`class_weight='balanced'` or custom weights).\n",
    "   - Or resample: oversample minority (SMOTE), or undersample majority, or combine (SMOTE+Tomek).\n",
    "6. **Regularization & hyperparameter tuning**:\n",
    "   - Tune `C` (inverse of regularization strength), penalty (`l1`, `l2`, `elasticnet`) using `GridSearchCV` or `RandomizedSearchCV`.\n",
    "   - Use `solver` compatible with chosen penalty (e.g., `'saga'` for `l1`).\n",
    "7. **Evaluation strategy**:\n",
    "   - Prefer metrics robust to imbalance: ROC-AUC, PR-AUC (precision-recall), F1 for chosen operating point.\n",
    "   - Use cross-validation stratified folds.\n",
    "   - Calibrate predicted probabilities if you need reliable probability estimates (Platt scaling / isotonic).\n",
    "8. **Business decisioning**:\n",
    "   - Choose threshold based on business metric (e.g., maximize expected profit or recall at a given precision).\n",
    "   - Run champion-challenger tests and A/B tests before deployment.\n",
    "9. **Monitoring**:\n",
    "   - Monitor model performance and data drift; keep feedback loop for retraining.\n",
    "\n",
    "This workflow balances statistical rigor with business needs for low-response-rate campaigns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242aee78",
   "metadata": {},
   "source": [
    "## End of assignment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
